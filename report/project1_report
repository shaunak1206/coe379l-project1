# COE 379L – Project 1 Report

Name: Shaunak Kapur
Course: COE 379L – Software Design for Responsible Intelligent Systems

------------------------------------------------------------

## 1) Data Preparation

To begin the project, I examined the dataset, which contained 131,165 rows and 12 columns
representing information about shelter animals and their outcomes. I started by cleaning the
data to ensure it was consistent and ready for modeling.

I first removed 17 duplicate rows and standardized column names for easier reference. Then,
I addressed missing values. For categorical variables (such as Sex upon Outcome and Animal Type),
I filled missing entries using the mode — the most frequent value — since this approach preserves
the distribution of each feature. Rows missing the target variable (Outcome Type) were removed, as
they could not contribute to model training.

The column Age upon Outcome, originally stored as a string (e.g., "2 years," "3 weeks"), was
converted into a numeric variable in days using a robust parsing function that handled various
time units (days, weeks, months, years). This conversion allowed for direct comparison across
animals of different ages and provided a standardized numeric feature for modeling.

For modeling purposes, I dropped several columns that were either irrelevant, redundant, or had
too many unique values (high cardinality), such as Animal ID, Date of Birth, DateTime, and MonthYear.
However, I kept Breed and Color columns for exploratory data analysis before dropping them
specifically for the modeling phase as required by the project specifications.

After these preprocessing steps, the resulting dataset was smaller, more balanced, and computationally
manageable, which helped prevent program crashes during training.

------------------------------------------------------------

## 2) Insights from Exploratory Data Analysis (EDA)

Before modeling, I conducted a comprehensive Exploratory Data Analysis (EDA) to understand key trends
and patterns in the shelter data. The analysis included univariate distributions, temporal trends,
and categorical breakdowns.

**Outcome Distribution**: Approximately 64% of animals were adopted, while 36% were transferred,
showing that adoption was the dominant outcome. This class imbalance was important to consider
during model evaluation.

**Animal Demographics**: The majority of animals were dogs and cats, with smaller groups consisting
of birds, rabbits, and other species. The top breeds included various mixed breeds and popular
purebreds, while the most common colors were black, brown, and white combinations.

**Temporal Patterns**: Analysis of outcomes over time suggested fluctuations in adoption rates, which 
could reflect seasonal or policy-driven trends.

**Age and Adoption**: A notable trend emerged regarding age: younger animals had a significantly
higher likelihood of adoption. Visualizing the age distribution confirmed that the probability
of adoption decreased as the animal's age increased, with a clear preference for younger pets.

**Sterilization Status**: The Sex upon Outcome feature provided meaningful insight. Animals that
were spayed or neutered had substantially higher adoption rates compared to those that were intact.
This may reflect adopter preferences for sterilized pets or shelter policies that promote adoption
of animals ready for home environments.

**Key Predictors**: Overall, these findings suggested that age, species, sterilization status,
and potentially breed and color were among the most influential predictors for adoption likelihood.

------------------------------------------------------------

## 3) Modeling Procedure

The primary goal was to build a model that could predict the animal's outcome type — specifically
whether an animal would be adopted or transferred.

I split the dataset into 80% training and 20% testing sets, using stratified sampling to preserve
the ratio of adoption to transfer outcomes in both subsets. This ensured that both training and
test sets maintained the same class distribution as the original dataset. For preprocessing, I used
a scikit-learn pipeline that handled numeric scaling, imputation for missing values, and one-hot
encoding of categorical features.

Three different models were trained and compared:
  1. **K-Nearest Neighbors (KNN) baseline** with k = 5, providing a simple non-parametric baseline
  2. **KNN with GridSearchCV** using a limited parameter grid (k = [3, 5, 7]) and only the core features
     (animal_type, sex_upon_outcome, age_in_days) to prevent computational crashes while still
     demonstrating hyperparameter optimization
  3. **Perceptron**, a linear classifier that provides a simple linear decision boundary and
     computational efficiency

The modeling approach was designed to balance performance with computational feasibility, using
single-threaded processing and reduced feature sets where necessary to prevent system crashes.
This modeling process allowed for both a simple baseline and a more interpretable linear model
for comparison.

------------------------------------------------------------

## 4) Model Performance and Metrics

The models were evaluated on the held-out test set using accuracy, precision, recall, and macro F1-score.
The comprehensive evaluation included detailed classification reports for each model:

**KNN (k = 5)**: Accuracy = 0.864, Precision = 0.863, Recall = 0.839, Macro F1 = 0.849
**KNN (GridSearchCV - Limited Features)**: Accuracy = 0.853, Precision = 0.861, Recall = 0.818, Macro F1 = 0.832  
**Perceptron**: Accuracy = 0.838, Precision = 0.827, Recall = 0.819, Macro F1 = 0.823

**How does the model perform to predict the class?**

All models demonstrate strong performance for this binary classification task, with accuracy ranging
from 0.83 to 0.86. The KNN baseline with k = 5 achieves the best overall performance by both accuracy
and macro F1-score, indicating balanced effectiveness across both adoption and transfer classes.

**Most Important Metric**: For this animal shelter outcome prediction problem, the **F1-score** is the
most important metric because:
1. It balances both precision and recall, providing a single comprehensive measure
2. In shelter operations, both false positives and false negatives have significant implications
3. False positives (predicting adoption when it's actually a transfer) waste resources and planning
4. False negatives (missing actual adoption opportunities) represent lost chances for animal welfare
5. F1-score provides a balanced view that considers both operational concerns

The Perceptron model offers a strong, interpretable linear alternative with computational efficiency,
while the KNN baseline provides the highest accuracy for applications where peak performance is critical.

------------------------------------------------------------

## 5) Confidence and Limitations

I am confident that the models provide effective solutions for predicting animal shelter outcomes.
The KNN baseline with k = 5 demonstrates the highest performance, while the Perceptron offers an
interpretable linear alternative with strong computational efficiency. Both models achieve accuracy
above 83%, indicating reliable predictive capability for this binary classification task.

**Key Strengths**:
- Robust data preprocessing with proper handling of missing values and data type conversions
- Comprehensive EDA that revealed important patterns in adoption likelihood
- Stratified train-test split ensuring representative evaluation
- Multiple model approaches providing different trade-offs between accuracy and interpretability

**Limitations and Considerations**:

1. **Feature Limitations**: To prevent computational crashes, the GridSearchCV model used only core
   features (animal_type, sex_upon_outcome, age_in_days). While this ensured stability, it may have
   limited the model's predictive power compared to using the full feature set.

2. **Computational Constraints**: The original GridSearchCV approach was modified to use single-threaded
   processing and reduced parameter grids to prevent system crashes. This represents a practical
   compromise between model optimization and computational feasibility.

3. **Generalization**: Since the dataset reflects a specific animal shelter (Austin), the learned
   patterns may not generalize to shelters with different populations, policies, or adoption practices.
   Regional differences, shelter policies, and demographic variations could affect model performance
   in other contexts.

4. **Temporal Considerations**: The dataset represents a historical snapshot, and adoption patterns
   may change over time due to evolving policies, economic conditions, or social trends.

**Future Improvements**: These limitations could be addressed by incorporating more computational
resources, using dimensionality reduction techniques for high-cardinality features, validating the
model on data from multiple shelters, and implementing temporal validation to assess model stability
over time.

------------------------------------------------------------

